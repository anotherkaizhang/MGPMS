{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import time\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(1)\n",
    "torch.cuda.set_device(device)\n",
    "torch.cuda.current_device()\n",
    "\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_rawdata(T, Y, ind_kf, ind_kt, X, meds_on_grid):\n",
    "    N = np.shape(T)[0] #num in batch\n",
    "\n",
    "    num_meds = np.shape(meds_on_grid[0])[1] \n",
    "    num_covs = np.shape(covs)[1]\n",
    "    \n",
    "    T_lens = np.array([len(t) for t in T])  \n",
    "    T_maxlen = np.max(T_lens)              \n",
    "    T_pad = np.zeros((N,T_maxlen))\n",
    "    \n",
    "    Y_lens = np.array([len(y) for y in Y])\n",
    "    Y_maxlen = np.max(Y_lens) \n",
    "    Y_pad = np.zeros((N,Y_maxlen))\n",
    "    ind_kf_pad = np.zeros((N,Y_maxlen))\n",
    "    ind_kt_pad = np.zeros((N,Y_maxlen))\n",
    "    \n",
    "    grid_lens = np.array([np.shape(m)[0] for m in meds_on_grid])  \n",
    "    grid_maxlen = np.max(grid_lens)                               \n",
    "    meds_pad = np.zeros((N,grid_maxlen,num_meds))\n",
    "    X_pad = np.zeros((N,grid_maxlen))\n",
    "    \n",
    "    for i in range(N):\n",
    "        T_pad[i,:T_lens[i]] = T[i]\n",
    "        Y_pad[i,:Y_lens[i]] = Y[i]\n",
    "        ind_kf_pad[i,:Y_lens[i]] = ind_kf[i]\n",
    "        ind_kt_pad[i,:Y_lens[i]] = ind_kt[i]\n",
    "        X_pad[i,:grid_lens[i]] = X[i]\n",
    "        meds_pad[i,:grid_lens[i],:] = meds_on_grid[i]\n",
    "                    \n",
    "    return T_pad, Y_pad, \\\n",
    "           ind_kf_pad, ind_kt_pad, \\\n",
    "           X_pad, meds_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OU_kernel(length,x1,x2):\n",
    "    x1 = torch.reshape(x1, (-1, 1)) \n",
    "    x2 = torch.reshape(x2, (1, -1)) \n",
    "    K = torch.exp(-torch.abs(x1 - x2)/length)\n",
    "    return K\n",
    "\n",
    "    \n",
    "def SE_kernel(length,x1,x2):\n",
    "    x1 = torch.reshape(x1, (-1, 1)) \n",
    "    x2 = torch.reshape(x2, (1, -1)) \n",
    "    K = torch.exp(-torch.pow(x1 - x2, 2.0) / length)\n",
    "    return K\n",
    "\n",
    "def gather_nd(K, ind):\n",
    "    ind = ind.type(torch.long)\n",
    "    return K[list(ind.T)].T\n",
    "\n",
    "def log_sum_exp(value, dim=None, keepdim=False):\n",
    "    if dim is not None:\n",
    "        m, _ = torch.max(value, dim=dim, keepdim=True)\n",
    "        value0 = value - m\n",
    "        if keepdim is False:\n",
    "            m = m.squeeze(dim)\n",
    "        return m + torch.log(torch.sum(torch.exp(value0),\n",
    "                                       dim=dim, keepdim=keepdim))\n",
    "    else:\n",
    "        m = torch.max(value)\n",
    "        sum_exp = torch.sum(torch.exp(value - m))\n",
    "        return m + torch.log(sum_exp)\n",
    "\n",
    "\n",
    "def get_probs_and_accuracy(preds, O):\n",
    "    all_probs = torch.exp(preds[:,1] - log_sum_exp(preds, dim = 1)) #normalize; and drop a dim so only prob of positive case\n",
    "    N = preds.shape[0] / n_mc_smps #actual number of observations in preds, collapsing MC samples                    \n",
    "    \n",
    "    probs = torch.zeros([0], device=device) #store all samples in a list, then concat into tensor at end\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    while i < N:\n",
    "        probs = torch.cat([probs, torch.tensor([torch.mean(all_probs[i*n_mc_smps : i*n_mc_smps+n_mc_smps])], device=device)], 0)\n",
    "        i += 1\n",
    "        \n",
    "    correct_pred = torch.eq(torch.gt(probs, 0.5).type(torch.uint8) , O) \n",
    "    accuracy = torch.mean((correct_pred.type(torch.float32))) \n",
    "    return probs, accuracy\n",
    "\n",
    "def CG(A, b):\n",
    "    \"\"\" Conjugate gradient, to get solution x = A^-1 * b,\n",
    "    can be faster than using the Cholesky for large scale problems\n",
    "    \"\"\"\n",
    "    b = torch.reshape(b, (-1,))\n",
    "    n = A.shape[0]\n",
    "    x = torch.zeros((n,), device=device) \n",
    "    r = b \n",
    "    p = r\n",
    "    \n",
    "    CG_EPS = n / 1000.0   \n",
    "    MAX_ITER = n / 250 + 3\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    while i < MAX_ITER and torch.norm(r) > CG_EPS:\n",
    "        p_vec = torch.reshape(p, (-1, 1))\n",
    "        Ap = torch.reshape(torch.mm(A, p_vec), (-1,))\n",
    "        alpha = torch.dot(r,r)/torch.dot(p, Ap)\n",
    "        x = x + alpha * p\n",
    "        r2 = r - alpha * Ap\n",
    "        beta = torch.dot(r2, r2)/torch.dot(r, r)\n",
    "        r = r2\n",
    "        p = r + beta * p\n",
    "        i += 1\n",
    "    \n",
    "    return torch.reshape(x,(-1,1))\n",
    "\n",
    "def block_CG(A_,B_):\n",
    "    \"\"\"\n",
    "    block version of CG. Get solution to matrix equation AX = B, ie\n",
    "    X = A^-1 * B. Will be much faster than Cholesky for large-scale problems.\n",
    "    \"\"\"\n",
    "    n = B_.shape[0]   \n",
    "    m = B_.shape[1]   \n",
    "    \n",
    "\n",
    "    X = torch.zeros((n,m), device=device)\n",
    "    V_ = torch.zeros((n,m), device=device)\n",
    "    R = B_\n",
    "    R_ = B_\n",
    "\n",
    "    CG_EPS = n / 1000.0 \n",
    "    MAX_ITER = n / 250 + 3\n",
    "    \n",
    "    i = 0\n",
    "    while i < MAX_ITER and torch.norm(R) > CG_EPS:\n",
    "        S = torch.solve(torch.mm(torch.transpose(R, 0, 1), R), \n",
    "                        torch.mm(torch.transpose(R_, 0, 1), R_))[0]\n",
    "        V = R + torch.mm(V_,S)\n",
    "        T = torch.solve(torch.mm(torch.transpose(R, 0, 1), R),\n",
    "                        torch.mm(torch.transpose(V, 0, 1), torch.mm(A_, V)))[0]\n",
    "        X = X + torch.mm(V,T)\n",
    "        V_ = V\n",
    "        R_ = R\n",
    "        R = R - torch.mm(A_,torch.mm(V,T))\n",
    "        i += 1\n",
    "    \n",
    "    return X\n",
    "\n",
    "def Lanczos(Sigma_func,b):\n",
    "    n = b.shape[0]\n",
    "    k = n / 500 + 3 \n",
    "\n",
    "    betas = torch.zeros(1, device=device)\n",
    "    alphas = torch.zeros(0, device=device)\n",
    "    D = torch.zeros((n, 1), device=device)\n",
    "    \n",
    "    b_norm = torch.norm(b)\n",
    "    D = torch.cat((D, torch.reshape(b / b_norm, (-1,1))), 1)\n",
    "    \n",
    "\n",
    "    j = 1\n",
    "    while j < k + 1:\n",
    "        d_j = D[:, j:j+1]\n",
    "        d = Sigma_func(d_j) - betas[-1] * D[:, j-1:j]\n",
    "        alphas = torch.cat((alphas, [torch.dot(d_j, d)]), 0)\n",
    "        d = d - alphas[-1] * d_j\n",
    "        betas = torch.cat((betas, [torch.norm(d)]), 0)\n",
    "        D = torch.cat((D, d/betes[j:j+1]), 1)\n",
    "        j += 1\n",
    "        \n",
    "        \n",
    "    betas_ = torch.diag(betas[1:k])\n",
    "    D_ = D[:,1:k+1]\n",
    "    \n",
    "    H = torch.diag(alphas) + F.pad(betas_, (0,1,1,0)) + F.pad(betas_, (1,0,0,1))\n",
    "    \n",
    "    e,v = torch.symeig(H, eigenvectors=True)   \n",
    "    e_pos = torch.max(0.0, e) + 1e-6        \n",
    "    e_sqrt = torch.diag(torch.sqrt(e_pos))\n",
    "    sq_H = torch.mm(v, torch.mm(e_sqrt, torch.transpose(v, 0, 1)))\n",
    "\n",
    "    out = b_norm * torch.mm(D_, sq_H) \n",
    "    return out[:, 0:1] \n",
    "\n",
    "\n",
    "def block_Lanczos(Sigma_func,B_,n_mc_smps):\n",
    "    \"\"\"\n",
    "    block Lanczos method to approx Sigma^1/2 * B, with B matrix of N(0,1)'s.\n",
    "    Used to generate multiple approximate large normal draws.\n",
    "    \n",
    "    \"\"\"\n",
    "    n = B_.shape[0] \n",
    "    s = n_mc_smps \n",
    "    k = int(n / 500 + 3) \n",
    "    \n",
    "    betas = torch.zeros((1,s), device=device)\n",
    "    alphas = torch.zeros((0,s), device=device)\n",
    "    D = torch.zeros((s,n,1), device=device)\n",
    "    \n",
    "    B_norms = torch.norm(B_, dim=0)\n",
    "    D = torch.cat((D, torch.unsqueeze(torch.transpose(B_/B_norms, 0, 1), 2)), 2)\n",
    "    \n",
    "    \n",
    "    j = 1\n",
    "    while j < k + 1:\n",
    "        d_j = torch.squeeze( D[:,:,j:j+1] )\n",
    "        d = Sigma_func(torch.transpose(d_j, 0, 1)) - betas[j-1:j, :]* \\\n",
    "                torch.transpose(torch.squeeze( D[:,:,j-1:j] ), 0, 1)\n",
    "        alphas = torch.cat((alphas, torch.diagonal(torch.mm(d_j,d)).unsqueeze(0)), 0)\n",
    "        d = d - alphas[j-1:j, :] * torch.transpose(d_j, 0, 1)\n",
    "        betas = torch.cat((betas, torch.norm(d, dim=0).unsqueeze(0)), 0)\n",
    "        D = torch.cat((D, torch.transpose(d/ betas[j:j+1, :], 0, 1).unsqueeze(2)), 2)\n",
    "        j += 1\n",
    "    \n",
    "\n",
    "    D_ =  D[:,:,1:1+k] \n",
    "    \n",
    "    H = torch.zeros((0, k, k), device=device)\n",
    "    \n",
    "    for ss in range(s):\n",
    "        this_beta = torch.diag(torch.squeeze(  betas[1:k, ss:ss+1]))\n",
    "        this_H = (torch.diag(torch.squeeze( alphas[:, ss:ss+1] )) +\n",
    "                  F.pad(this_beta, (0, 1, 1, 0)) +\n",
    "                   F.pad(this_beta, (1, 0, 0, 1)))\n",
    "        H = torch.cat((H, this_H.unsqueeze(0)),0)    \n",
    "    \n",
    "    E, V = torch.symeig(H, eigenvectors=True) # !!!different from 'torch.eig'\n",
    "    E_sqrt = torch.zeros((0,k,k), device=device)\n",
    "\n",
    "    for ss in range(s): \n",
    "        E_sqrt = torch.cat((E_sqrt, torch.diag(torch.squeeze(torch.sqrt(torch.max( E[ss:ss+1, :] , 1e-6*torch.ones_like(E[ss:ss+1, :], device=device) )))).unsqueeze(0)), 0)\n",
    "    \n",
    "    \n",
    "    sq_H = torch.matmul(V, torch.matmul(E_sqrt, V.permute(0, 2, 1)))\n",
    "        \n",
    "    e1 = torch.transpose(torch.eye(k, device=device)[:,0:1].repeat(1, s), 0, 1).unsqueeze(2)  # !different from torch.eye(k)[:,0], one is vertical, one is horizontal\n",
    "    \n",
    "    \n",
    "    out = B_norms * torch.transpose(torch.squeeze(torch.matmul(D_, torch.matmul(sq_H, e1))), 0, 1)\n",
    "    return out\n",
    "\n",
    "def draw_GP(Yi, Ti, Xi, ind_kfi, ind_kti, length, noises, Kf, n_mc_smps):\n",
    "    \"\"\" \n",
    "    given GP hyperparams and data values at observation times, draw from \n",
    "    conditional GP\n",
    "\n",
    "    inputs:\n",
    "        length,noises,Lf,Kf: GP params\n",
    "        Yi: observation values\n",
    "        Ti: observation times\n",
    "        Xi: grid points (new times for rnn)\n",
    "        ind_kfi,ind_kti: indices into Y\n",
    "    returns:\n",
    "        draws from the GP at the evenly spaced grid times Xi, given hyperparams and data\n",
    "    \"\"\"  \n",
    "    ny = Yi.shape[0]\n",
    "    K_tt = OU_kernel(length, Ti, Ti)\n",
    "    \n",
    "    D = torch.diag(noises)\n",
    "        \n",
    "    grid_f = torch.meshgrid(ind_kfi, ind_kfi) \n",
    "    grid_f = (grid_f[0].T, grid_f[1].T)\n",
    "    \n",
    "    Kf_big = gather_nd(Kf, torch.stack((grid_f[0],grid_f[1]),-1))\n",
    "\n",
    "    grid_t = torch.meshgrid(ind_kti, ind_kti) \n",
    "    grid_t = (grid_t[0].T, grid_t[1].T)\n",
    "    Kt_big = gather_nd(K_tt, torch.stack((grid_t[0],grid_t[1]),-1))\n",
    "\n",
    "    Kf_Ktt = torch.mul(Kf_big,Kt_big)\n",
    "\n",
    "    DI_big = gather_nd(D,torch.stack((grid_f[0],grid_f[1]),-1))\n",
    "    DI = torch.diag(torch.diagonal(DI_big, dim1=-2, dim2=-1)) \n",
    "\n",
    "    \n",
    "    #data covariance. \n",
    "    #Either need to take Cholesky of this or use CG / block CG for matrix-vector products\n",
    "    Ky = Kf_Ktt + DI + 1e-6 * torch.eye(ny, device = device)\n",
    "\n",
    "    ### build out cross-covariances and covariance at grid\n",
    "    nx = Xi.shape[0]\n",
    "\n",
    "    K_xx = OU_kernel(length,Xi,Xi)\n",
    "    K_xt = OU_kernel(length,Xi,Ti)\n",
    "\n",
    "    ind = torch.cat([ torch.tensor([i], device=device).repeat([nx]) for i in range(M)], 0)\n",
    "    grid = torch.meshgrid(ind, ind) #indexing=xy,\n",
    "    grid = (grid[0].T, grid[1].T)\n",
    "    Kf_big = gather_nd(Kf, torch.stack((grid[0],grid[1]), -1))\n",
    "    ind2 = torch.arange(0, nx, device=device).repeat([M])\n",
    "    grid2 = torch.meshgrid(ind2, ind2) #indexing=xy,\n",
    "    grid2 = (grid2[0].T, grid2[1].T)\n",
    "    Kxx_big = gather_nd(K_xx, torch.stack((grid2[0], grid2[1]),-1))\n",
    "\n",
    "    K_ff = torch.mul(Kf_big,Kxx_big)       \n",
    "\n",
    "    full_f = torch.cat([ torch.tensor([i], device=device).repeat([nx]) for i in range(M)],0)\n",
    "    grid_1 = torch.meshgrid(full_f,ind_kfi)  #indexing=ij,\n",
    "    Kf_big = gather_nd(Kf, torch.stack((grid_1[0],grid_1[1]),-1))\n",
    "    full_x = torch.arange(0, nx, device=device).repeat([M]).type(torch.long)\n",
    "\n",
    "    grid_2 = torch.meshgrid(full_x,ind_kti) #indexing=ij,\n",
    "    Kxt_big = gather_nd(K_xt, torch.stack((grid_2[0],grid_2[1]),-1))\n",
    "\n",
    "    K_fy = torch.mul(Kf_big, Kxt_big)\n",
    "    \n",
    "    \n",
    "    #now get draws!\n",
    "    y_ = torch.reshape(Yi, (-1,1))\n",
    "\n",
    "\n",
    "    Mu = torch.matmul(K_fy,CG(Ky,y_)) #May be faster with CG for large problems\n",
    "    Ly = torch.cholesky(Ky) # Compute Ly uses greatly time >> Mu\n",
    "#     Mu = torch.mm(K_fy, torch.cholesky_solve(y_, Ly))  # in tensorflow: tf.cholesky_solve(Ly, y_)\n",
    "    \n",
    "    xi = torch.normal(mean=0, std=1.0, size=(nx*M, n_mc_smps), device=device)\n",
    "    Sigma = K_ff - torch.mm(K_fy, torch.cholesky_solve(torch.transpose(K_fy, 0, 1), Ly)) + 1e-6 * torch.eye(K_ff.shape[0], device=device)\n",
    "\n",
    "    '''\n",
    "    #Never need to explicitly compute Sigma! Just need matrix products with Sigma in Lanczos algorithm\n",
    "    def Sigma_mul(vec):\n",
    "        # vec must be a 2d tensor, shape (?,?) \n",
    "        return torch.mm(K_ff, vec) - torch.mm(K_fy,block_CG(Ky,torch.mm(torch.transpose(K_fy, 0, 1),vec))) \n",
    "\n",
    "    def small_draw():   \n",
    "        return Mu + torch.mm(torch.cholesky(Sigma),xi)\n",
    "    def large_draw():             \n",
    "        return Mu + block_Lanczos(Sigma_mul,xi,n_mc_smps) #no need to explicitly reshape Mu\n",
    "\n",
    "    BLOCK_LANC_THRESH = 10000\n",
    "    draw = small_draw() if nx * M < BLOCK_LANC_THRESH else large_draw() \n",
    "    '''\n",
    "\n",
    "    draw = Mu + torch.mm(torch.cholesky(Sigma), xi)\n",
    "    draw_reshape = (torch.reshape(torch.transpose(draw, 0, 1), (n_mc_smps, M, nx))).permute(0,2,1)\n",
    "    return draw_reshape   \n",
    "\n",
    "def get_GP_samples(Y, T, X, ind_kf, ind_kt, num_obs_times, num_obs_values,\n",
    "                   num_rnn_grid_times, med_grid,\n",
    "                   length, noises, Kf, \n",
    "                   n_mc_smps, M): \n",
    "    \"\"\"\n",
    "    returns samples from GP at evenly-spaced gridpoints\n",
    "    \"\"\" \n",
    "    # X is the batch-padded data, patients have the same sequence length\n",
    "    Z = torch.zeros((0, sequence_len, M), device=device)  # M = M(original) + n_meds\n",
    "        \n",
    "    N = T.shape[0] # number of observations\n",
    "\n",
    "    # change indices to 'long'\n",
    "    ind_kf = ind_kf.type(torch.long)\n",
    "    ind_kt = ind_kt.type(torch.long)\n",
    "\n",
    "\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        Yi = torch.reshape( Y[i:i+1, 0:num_obs_values[i]], (-1,))\n",
    "        Ti = torch.reshape( T[i:i+1, 0:num_obs_times[i]], (-1,))\n",
    "        ind_kfi = torch.reshape( ind_kf[i:i+1, 0:num_obs_values[i]], (-1,))\n",
    "        ind_kti = torch.reshape( ind_kt[i:i+1, 0:num_obs_values[i]], (-1,))\n",
    "        Xi = torch.reshape( X[i:i+1, 0:num_rnn_grid_times[i]], (-1,))\n",
    "        X_len = num_rnn_grid_times[i]    \n",
    "        \n",
    "        GP_draws = draw_GP(Yi, Ti, Xi, ind_kfi, ind_kti, length, noises, Kf, n_mc_smps)   \n",
    "        pad_len = sequence_len - X_len        #Time-axis: pad by this much\n",
    "        cur_GP_draw = torch.zeros((n_mc_smps, pad_len, GP_draws.shape[2]), dtype=torch.float32, device=device)\n",
    "        padded_GP_draws = torch.cat((GP_draws, cur_GP_draw), 1)  # pad on time-axis\n",
    "\n",
    "        meds = torch.tensor(med_grid[i:i+1], device=device) # [1, sequence_len, n_meds]  \n",
    "        pad_len = sequence_len - meds.shape[1]\n",
    "        meds = torch.cat([meds,torch.zeros((1,pad_len,meds.shape[2])).cuda(device=device)],1) \n",
    "        tiled_meds = meds.repeat(n_mc_smps, 1, 1)\n",
    "\n",
    "        padded_GPdraws_medcovs = torch.cat((padded_GP_draws, tiled_meds), 2)\n",
    "        Z = torch.cat((Z, padded_GPdraws_medcovs), 0)    \n",
    "        i += 1\n",
    "\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ninput, n_covs, sequence_len, emsize, nhead, nhid, nlayers, n_mc_smps, dropout=0.5): # ninput = M(original) + n_meds\n",
    "        super(TransformerModel, self).__init__()\n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.encoder = nn.Linear(ninput, emsize)\n",
    "        self.pos_encoder = PositionalEncoding(emsize, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(emsize, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.emsize = emsize\n",
    "        self.final = torch.rand(size=(sequence_len*(emsize+n_covs),))\n",
    "        # !!! Needs to check if final_reshape is still diagonal\n",
    "    \n",
    "        # GP parameters\n",
    "        self.M = ninput # self.M = ninput = M + n_meds\n",
    "        self.n_mc_smps = n_mc_smps\n",
    "        self.sequence_len = sequence_len\n",
    "        self.n_covs = n_covs\n",
    "        self.emsize = emsize\n",
    "        \n",
    "        #in fully separable case all labs share same time-covariance\n",
    "        self.log_length = torch.normal(size=[1], mean=1, std=0.1)\n",
    "        self.log_noises = torch.normal(size=[self.M], mean=-2, std=0.1)\n",
    "        self.L_f_init = torch.eye(self.M)\n",
    "        \n",
    "        # Wrap into Variable\n",
    "        self.log_length = torch.nn.Parameter(self.log_length)\n",
    "        self.log_noises = torch.nn.Parameter(self.log_noises)\n",
    "        self.L_f_init = torch.nn.Parameter(self.L_f_init)\n",
    "        self.final =  torch.nn.Parameter(self.final)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, Y, T, X, ind_kf, ind_kt, num_obs_times, num_obs_values,\n",
    "                   num_rnn_grid_times, med_grid, covs):      # [batch_size, sequence_len, M]\n",
    "\n",
    "        \n",
    "        length = torch.exp(self.log_length)\n",
    "        noises = torch.exp(self.log_noises)\n",
    "        Lf = torch.tril(self.L_f_init)  \n",
    "        Kf = torch.mm(Lf, torch.transpose(Lf, 0, 1))  \n",
    "        final_reshape = torch.zeros(size=(self.sequence_len*(self.emsize+self.n_covs), self.sequence_len)).to(device)\n",
    "        for i in range(self.sequence_len):\n",
    "            final_reshape[:2*(i+1), i] = self.final[:2*(i+1)]\n",
    "        \n",
    "        ### Draw samples\n",
    "        Z = get_GP_samples(Y=Y, \n",
    "                            T=T, \n",
    "                            X=X, \n",
    "                            ind_kf=ind_kf, \n",
    "                            ind_kt=ind_kt, \n",
    "                            num_obs_times=num_obs_times, \n",
    "                            num_obs_values=num_obs_values,\n",
    "                            num_rnn_grid_times=num_rnn_grid_times, \n",
    "                            med_grid=med_grid,\n",
    "                            length=length, \n",
    "                            noises=noises,\n",
    "                            Kf=Kf, \n",
    "                            n_mc_smps=self.n_mc_smps, \n",
    "                            M=self.M) \n",
    "                                \n",
    "                            \n",
    "        # Get shape \n",
    "        batch_size_MonteCarlo, _, _ = Z.shape\n",
    "        batch_size = batch_size_MonteCarlo // self.n_mc_smps\n",
    "                                                \n",
    "        # Enbedding       \n",
    "        src = self.encoder(Z)  # [batch_size_MC, sequence_len, embed_size]\n",
    "        src = src.permute(1,0,2) # [sequence_len, batch_size_MC, embed_size]\n",
    "                       \n",
    "        # Position-Encoder\n",
    "        src = self.pos_encoder(src)   # [sequence_len, batch_size_MC, embed_size] \n",
    "                          \n",
    "        # Transformer\n",
    "        output = self.transformer_encoder(src, self.src_mask)   # [sequence_len, batch_size_MC, embed_size] \n",
    "\n",
    "        # Append covs\n",
    "        output = output.permute(1,0,2)   # [batch_size_MC, sequence_len, embed_size]    \n",
    "        covs = covs.repeat((1,self.n_mc_smps)).repeat((1,self.sequence_len)).view((batch_size*self.n_mc_smps, self.sequence_len, self.n_covs)) # [batch_size_MC, sequence_len, ncovs] \n",
    "\n",
    "        output = torch.cat((output, covs), axis=2) # [batch_size_MC, sequence_len, embed_size+ncovs] \n",
    "    \n",
    "        output = torch.reshape(output, (batch_size*self.n_mc_smps, -1))   # [batch_size_MC, sequence_len *(embed_size+ncovs)] \n",
    "        \n",
    "        \n",
    "        output = torch.reshape(output, (batch_size*self.n_mc_smps, self.sequence_len*(self.emsize + self.n_covs)))  # [batch_size, sequence_len*(embed_size+ncovs)] row-by-row\n",
    "        output = torch.mm(output, final_reshape)   # [batch_size, sequenceLen]\n",
    "\n",
    "        # Take mean for Monte Carlo\n",
    "        output = torch.mean(output.reshape(-1, self.n_mc_smps, self.sequence_len), dim=1)  # [real_batch_size, n_mc_smps, sequenceLen]  \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"input_for_GPRNN-240minutes-168h_2020-10-27.pickle\", 'rb')\n",
    "\n",
    "input_for_GPRNN = pickle.load(f, encoding=\"latin1\")  \n",
    "f.close()\n",
    "\n",
    "num_obs_times = input_for_GPRNN['num_obs_times']\n",
    "num_obs_values = input_for_GPRNN['num_obs_values']\n",
    "num_rnn_grid_times = input_for_GPRNN['num_rnn_grid_times']\n",
    "rnn_grid_times = input_for_GPRNN['rnn_grid_times']\n",
    "labels = input_for_GPRNN['labels']\n",
    "times = input_for_GPRNN['times']\n",
    "values = input_for_GPRNN['values']\n",
    "ind_lvs = input_for_GPRNN['ind_lvs']\n",
    "ind_times = input_for_GPRNN['ind_times']\n",
    "meds_on_grid = input_for_GPRNN['meds_on_grid']\n",
    "covs = input_for_GPRNN['covs']\n",
    "\n",
    "print(\"That's all!\")\n",
    "\n",
    "N_tot = len(labels) # total number of patients\n",
    "\n",
    "seed = 8675309\n",
    "rs = np.random.RandomState(seed) #fixed seed in np\n",
    "\n",
    "    \n",
    "train_test_perm = rs.permutation(N_tot)\n",
    "val_frac = 0.1 #fraction of full data to set aside for testing\n",
    "\n",
    "te_ind = train_test_perm[: int(val_frac*N_tot)]\n",
    "tr_ind = train_test_perm[int(val_frac*N_tot) :]\n",
    "\n",
    "Nte = len(te_ind)\n",
    "Ntr = len(tr_ind)\n",
    "\n",
    "batch_size = 50\n",
    "eval_batch_size = 10\n",
    "\n",
    "starts_tr = np.arange(0, Ntr, batch_size)\n",
    "ends_tr = np.arange(batch_size, Ntr + 1, batch_size)\n",
    "\n",
    "if len(starts_tr) > len(ends_tr):\n",
    "    starts_tr = starts_tr[:-1]\n",
    "\n",
    "starts_te = np.arange(0, Nte, eval_batch_size)\n",
    "ends_te = np.arange(eval_batch_size, Nte + 1, eval_batch_size)\n",
    "\n",
    "if len(starts_te) > len(ends_te):\n",
    "    starts_te = starts_te[:-1]\n",
    "\n",
    "\n",
    "# Break everything out into train/test\n",
    "for varname in ['covs', 'labels', 'times', 'values', 'ind_lvs', 'ind_times', 'meds_on_grid', \\\n",
    "               'num_obs_times', 'num_obs_values', 'rnn_grid_times', 'num_rnn_grid_times']:\n",
    "    print(varname + '_tr = [' + varname + '[i] for i in tr_ind]')\n",
    "    exec(varname + '_tr = [' + varname + '[i] for i in tr_ind]')\n",
    "\n",
    "    \n",
    "for varname in ['covs', 'labels', 'times', 'values', 'ind_lvs', 'ind_times', 'meds_on_grid', \\\n",
    "               'num_obs_times', 'num_obs_values', 'rnn_grid_times', 'num_rnn_grid_times']:\n",
    "    print(varname + '_te = [' + varname + '[i] for i in te_ind]')\n",
    "    exec(varname + '_te = [' + varname + '[i] for i in te_ind]')\n",
    "    \n",
    "print(\"data fully setup!\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 25       # Number of features (labs + vitals)\n",
    "n_covs = 33  # Number of Covariants \n",
    "n_meds = 21  # Number of Medicines\n",
    "\n",
    "ninput = M + n_meds  \n",
    "emsize = 512 # embedding dimension\n",
    "nhid = 2048 #  the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 6 #  the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 8   #  the number of heads in the multiheadattention models\n",
    "dropout = 0.3 # the dropout value\n",
    "\n",
    "\n",
    "sequence_len = 42  # the maximum sampling time points of all patients\n",
    "n_mc_smps = 20     # Monte Carlo Samples\n",
    "\n",
    "model = TransformerModel(ninput=ninput, \n",
    "                         n_covs=n_covs, \n",
    "                         sequence_len=sequence_len, \n",
    "                         emsize=emsize, \n",
    "                         nhead=nhead, \n",
    "                         nhid=nhid, \n",
    "                         nlayers=nlayers,\n",
    "                         n_mc_smps=n_mc_smps, \n",
    "                         dropout=dropout).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss(reduction='sum')\n",
    "lr = 0.03  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    \n",
    "    perm = rs.permutation(Ntr)\n",
    "    \n",
    "    batch = 0 \n",
    "    for s, e in zip(starts_tr, ends_tr):\n",
    "        batch_start = time.time()\n",
    "        inds = perm[s:e]\n",
    "            \n",
    "        \n",
    "        T_pad,  Y_pad,  ind_kf_pad,\\\n",
    "        ind_kt_pad, X_pad,  meds_pad \\\n",
    "                        = pad_rawdata(T = [times_tr[i] for i in inds],\n",
    "                                      Y = [values_tr[i] for i in inds], \n",
    "                                      ind_kf = [ind_lvs_tr[i] for i in inds], \n",
    "                                      ind_kt = [ind_times_tr[i] for i in inds], \n",
    "                                      X = [rnn_grid_times_tr[i] for i in inds],\n",
    "                                      meds_on_grid = [meds_on_grid_tr[i] for i in inds])\n",
    "                                      \n",
    "        ### Move data to tensor \n",
    "        T_pad = torch.tensor(T_pad, dtype=torch.float32)\n",
    "        Y_pad = torch.tensor(Y_pad,  dtype=torch.float32)\n",
    "        X_pad = torch.tensor(X_pad,  dtype=torch.float32)\n",
    "        ind_kf_pad = torch.tensor(ind_kf_pad,  dtype=torch.int32)\n",
    "        ind_kt_pad = torch.tensor(ind_kt_pad,  dtype=torch.int32)\n",
    "        meds_pad =torch.tensor(meds_pad,  dtype=torch.float32)\n",
    "        covs = torch.tensor([covs_tr[i] for i in inds],  dtype=torch.float32)\n",
    "        num_obs_times = torch.tensor([num_obs_times_tr[i] for i in inds], dtype=torch.int32)\n",
    "        num_obs_values= torch.tensor([num_obs_values_tr[i] for i in inds], dtype=torch.int32)\n",
    "        num_rnn_grid_times=torch.tensor([num_rnn_grid_times_tr[i] for i in inds], dtype=torch.int32)\n",
    "        O = torch.tensor([labels_tr[i] for i in inds], dtype=torch.float32)\n",
    "        O_dup = torch.reshape(O.unsqueeze(1).repeat(1, sequence_len), (-1,))\n",
    "        \n",
    "        \n",
    "        ## Move to device\n",
    "        T_pad = T_pad.to(device)\n",
    "        Y_pad = Y_pad.to(device)\n",
    "        X_pad = X_pad.to(device)\n",
    "        ind_kf_pad = ind_kf_pad.to(device)\n",
    "        ind_kt_pad = ind_kt_pad.to(device)\n",
    "        meds_pad =meds_pad.to(device)\n",
    "        covs = covs.to(device)\n",
    "        num_obs_times = num_obs_times.to(device)\n",
    "        num_obs_values = num_obs_values.to(device)\n",
    "        num_rnn_grid_times = num_rnn_grid_times.to(device)\n",
    "        O = O.to(device)\n",
    "        O_dup = torch.reshape(O.unsqueeze(1).repeat(1, sequence_len), (-1,))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(Y_pad, T_pad, X_pad, ind_kf_pad, ind_kt_pad, num_obs_times, num_obs_values,\n",
    "                   num_rnn_grid_times, meds_pad, covs)   # [batch_size, sequence_len]\n",
    "        loss = criterion(output.view(-1), O_dup)\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = 1\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f}'.format(\n",
    "                    epoch, batch,  len(starts_tr), scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "        batch += 1\n",
    "        \n",
    "        \n",
    "def evaluate(eval_model):\n",
    "    eval_model.eval() \n",
    "    total_loss = 0.\n",
    "\n",
    "    perm = rs.permutation(Nte)\n",
    "    \n",
    "    \n",
    "    output_all = []\n",
    "    target_all = []\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for s, e in tqdm(zip(starts_te, ends_te)):\n",
    "            batch_start = time.time()\n",
    "            inds = perm[s:e]\n",
    "            \n",
    "            T_pad,  Y_pad,  ind_kf_pad,\\\n",
    "                ind_kt_pad, X_pad,  meds_pad \\\n",
    "                        = pad_rawdata(T = [times_te[i] for i in inds],\n",
    "                                      Y = [values_te[i] for i in inds], \n",
    "                                      ind_kf = [ind_lvs_te[i] for i in inds], \n",
    "                                      ind_kt = [ind_times_te[i] for i in inds], \n",
    "                                      X = [rnn_grid_times_te[i] for i in inds],\n",
    "                                      meds_on_grid = [meds_on_grid_te[i] for i in inds])\n",
    "                                      \n",
    "\n",
    "            ### Move data to tensor \n",
    "            T_pad = torch.tensor(T_pad, dtype=torch.float32)\n",
    "            Y_pad = torch.tensor(Y_pad,  dtype=torch.float32)\n",
    "            X_pad = torch.tensor(X_pad,  dtype=torch.float32)\n",
    "            ind_kf_pad = torch.tensor(ind_kf_pad,  dtype=torch.int32)\n",
    "            ind_kt_pad = torch.tensor(ind_kt_pad,  dtype=torch.int32)\n",
    "            meds_pad =torch.tensor(meds_pad,  dtype=torch.float32)\n",
    "            covs = torch.tensor([covs_te[i] for i in inds],  dtype=torch.float32)\n",
    "            num_obs_times = torch.tensor([num_obs_times_te[i] for i in inds], dtype=torch.int32)\n",
    "            num_obs_values= torch.tensor([num_obs_values_te[i] for i in inds], dtype=torch.int32)\n",
    "            num_rnn_grid_times=torch.tensor([num_rnn_grid_times_te[i] for i in inds], dtype=torch.int32)\n",
    "            O = torch.tensor([labels_te[i] for i in inds], dtype=torch.float32)\n",
    "            O_dup = torch.reshape(O.unsqueeze(1).repeat(1, sequence_len), (-1,))\n",
    "            \n",
    "        \n",
    "            ### Move to device\n",
    "            T_pad = T_pad.to(device)\n",
    "            Y_pad = Y_pad.to(device)\n",
    "            X_pad = X_pad.to(device)\n",
    "            ind_kf_pad = ind_kf_pad.to(device)\n",
    "            ind_kt_pad = ind_kt_pad.to(device)\n",
    "            meds_pad =meds_pad.to(device)\n",
    "            covs = covs.to(device)\n",
    "            num_obs_times = num_obs_times.to(device)\n",
    "            num_obs_values = num_obs_values.to(device)\n",
    "            num_rnn_grid_times = num_rnn_grid_times.to(device)\n",
    "            O = O.to(device)\n",
    "            O_dup = torch.reshape(O.unsqueeze(1).repeat(1, sequence_len), (-1,))\n",
    "\n",
    "            output = model(Y_pad, T_pad, X_pad, ind_kf_pad, ind_kt_pad, num_obs_times, num_obs_values,\n",
    "                   num_rnn_grid_times, meds_pad, covs)\n",
    "\n",
    "            \n",
    "            output_all.extend(list(output.cpu().numpy()))\n",
    "            target_all.extend(list(O.cpu().numpy()))\n",
    "            \n",
    "            total_loss += criterion(output.view(-1), O_dup).item()\n",
    "            \n",
    "    return total_loss, output_all, target_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "epochs = 100\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train()\n",
    "    val_loss, _, _ = evaluate(model)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} '\n",
    "          .format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss))\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_loss, output_all, target_all = evaluate(best_model)\n",
    "print('-' * 89)\n",
    "print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} '\n",
    "      .format(epoch, (time.time() - epoch_start_time),\n",
    "                                 val_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
